{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Домашнее задание выполнил Ионкин К.А.\n",
    "# тема - Neural Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transform as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import copy\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перенесем необходимый функцонал с  семинарского ноутбука"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# естественно необохимо работат на GPU для ускорения процесса обучения\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# перед подачей изображения в модель, необходимо проихвести предобработку\n",
    "imsize = 128  \n",
    "\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize(imsize), \n",
    "    transforms.CenterCrop(imsize),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "# функця отрисовки изображений\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()   \n",
    "    image = image.squeeze(0)\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем фотки стилей и фотку контента\n",
    "style1_img = \n",
    "style2_img =\n",
    "content_img = \n",
    "\n",
    "# и сразу же отобразим их\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем классы StyleLoss и ContentLoss\n",
    "\n",
    "# класс ContentLoss остается без изменений\n",
    "class ContentLoss(nn.Module):\n",
    "\n",
    "        def __init__(self, target,):\n",
    "            super(ContentLoss, self).__init__()\n",
    "            # we 'detach' the target content from the tree used\n",
    "            # to dynamically compute the gradient: this is a stated value,\n",
    "            # not a variable. Otherwise the forward method of the criterion\n",
    "            # will throw an error.\n",
    "            self.target = target.detach()#это константа. Убираем ее из дерева вычеслений\n",
    "            self.loss = F.mse_loss(self.target, self.target )#to initialize with something\n",
    "\n",
    "        def forward(self, input_image):\n",
    "            self.loss = F.mse_loss(input_image, self.target)\n",
    "            \n",
    "            return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция получения матрицы грамма\n",
    "def gram_matrix(input_image):\n",
    "        batch_size, f_map_num, h, w  = input_image.size()  # batch size(=1)\n",
    "        \n",
    "        features = input_image.view(batch_size * f_map_num, h * w) \n",
    "\n",
    "        G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "        # we 'normalize' the values of the gram matrix\n",
    "        # by dividing by the number of element in each feature maps.\n",
    "        return G.div(batch_size * f_map_num * h * w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    h - horizontal split\n",
    "    V - vertical split\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_feature, mode_split=\"h\", style=\"style1\"):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "        self.loss = F.mse_loss(self.target, self.target)\n",
    "        self.mode_split = mode_split\n",
    "        self.style = style\n",
    "\n",
    "    def forward(self, input_image):\n",
    "        \n",
    "        mask_image = StyleLoss.split_dict[self.mode_split](self, input_image)\n",
    "        \n",
    "        G = gram_matrix(mask_image)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        \n",
    "        return input_image\n",
    "    \n",
    "    def horizontal_split(self, input_image):\n",
    "        \n",
    "        mask = torch.zeros_like(input_image)\n",
    "        _, _, h, _ = mask.size()\n",
    "        \n",
    "        if self.style == \"style1\":\n",
    "            mask[:, :, :h//2, :] +=1\n",
    "            \n",
    "        if self.style2 == \"style2\":\n",
    "            mask[:, :, h//2:, :] +=1\n",
    "        \n",
    "        return input_image * mask\n",
    "    \n",
    "    def vertical_split(self, input_image):\n",
    "        \n",
    "        mask = torch.zeros_like(input_image)\n",
    "        _, _, _, w = mask.size()\n",
    "        \n",
    "        if self.style == \"style1\":\n",
    "            mask[:, :, :, :w//2] +=1\n",
    "            \n",
    "        if self.style2 == \"style2\":\n",
    "            mask[:, :, :, w//2:] +=1\n",
    "        \n",
    "        return input_image * mask\n",
    "    \n",
    "    split_dict = {\n",
    "        \"h\": horizontal_split,  \n",
    "        \"v\": vertical_split\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers_default = ['conv_4'] # слой для вывода content_loss\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5'] # слои для вывода соответствующего style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "        def __init__(self, mean, std):\n",
    "            super(Normalization, self).__init__()\n",
    "            # .view the mean and std to make them [C x 1 x 1] so that they can\n",
    "            # directly work with image Tensor of shape [B x C x H x W].\n",
    "            # B is batch size. C is number of channels. H is height and W is width.\n",
    "            self.mean = mean.view(-1, 1, 1)\n",
    "            self.std = std.view(-1, 1, 1)\n",
    "\n",
    "        def forward(self, img):\n",
    "            # normalize img\n",
    "            return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь в нашей модели подряд идут 2 слоя StyleLoss для каждого из стилей\n",
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                                   style1_img, style2_img, content_img, mode_split=\"h\",\n",
    "                                   content_layers=content_layers_default,\n",
    "                                   style_layers=style_layers_default):\n",
    "        cnn = copy.deepcopy(cnn)\n",
    "\n",
    "        # normalization module\n",
    "        normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\n",
    "        # just in order to have an iterable access to or list of content/syle\n",
    "        # losses\n",
    "        content_losses = []\n",
    "        style1_losses = []\n",
    "        style2_losses = []\n",
    "\n",
    "        # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n",
    "        # to put in modules that are supposed to be activated sequentially\n",
    "        model = nn.Sequential(normalization) # мы создаем нашу модель на основе модели vgg19, но сначала нормализуем входный данные, так как в vgg19 поступают норм. данные\n",
    "\n",
    "        i = 0  # increment every time we see a conv\n",
    "        for layer in cnn.children():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                i += 1\n",
    "                name = 'conv_{}'.format(i)\n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                name = 'relu_{}'.format(i)\n",
    "                # The in-place version doesn't play very nicely with the ContentLoss\n",
    "                # and StyleLoss we insert below. So we replace with out-of-place\n",
    "                # ones here.\n",
    "                #Переопределим relu уровень\n",
    "                layer = nn.ReLU(inplace=False)\n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                name = 'pool_{}'.format(i)\n",
    "            elif isinstance(layer, nn.BatchNorm2d):\n",
    "                name = 'bn_{}'.format(i)\n",
    "            else:\n",
    "                raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "            model.add_module(name, layer)\n",
    "\n",
    "            if name in content_layers:\n",
    "                # add content loss:\n",
    "                target = model(content_img).detach()\n",
    "                content_loss = ContentLoss(target)\n",
    "                model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "                content_losses.append(content_loss)\n",
    "\n",
    "            if name in style_layers:\n",
    "                # add style1 loss\n",
    "                target1_feature = model(style1_img).detach()\n",
    "                style1_loss = StyleLoss(target1_feature, mode_split, style=\"style1\") # TO DO\n",
    "                model.add_module(\"style1_loss_{}\".format(i), style1_loss)\n",
    "                style1_losses.append(style1_loss)\n",
    "                \n",
    "                # add style2 loss\n",
    "                target2_feature = model(style2_img).detach()\n",
    "                style2_loss = StyleLoss(target2_feature, mode_split, style=\"style2\") # TO DO\n",
    "                model.add_module(\"style2_loss_{}\".format(i), style2_loss)\n",
    "                style2_losses.append(style2_loss)\n",
    "\n",
    "        # now we trim off the layers after the last content and style losses\n",
    "        #выбрасываем все уровни после последенего style loss или content loss\n",
    "        for i in range(len(model) - 1, -1, -1):\n",
    "            if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "                break\n",
    "\n",
    "        model = model[:(i + 1)]\n",
    "\n",
    "        return model, style1_losses, style2_losses, content_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_optimizer(input_img):\n",
    "        # this line to show that input is a parameter that requires a gradient\n",
    "        #добоваляет содержимое тензора катринки в список изменяемых оптимизатором параметров\n",
    "        optimizer = optim.LBFGS([input_img.requires_grad_()]) \n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                        input_img, style1_img, style2_img, content_img, mode_split=\"h\", num_steps=500,\n",
    "                        style1_weight=1e6, style2_weight=1e6 content_weight=1):\n",
    "        \"\"\"Run the style transfer.\"\"\"\n",
    "        \n",
    "        print('Building the style transfer model..')\n",
    "        model, style1_losses, style2_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "            normalization_mean, normalization_std, style1_img, style2_img, content_img, mode_split)\n",
    "        optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "        print('Optimizing..')\n",
    "        run = [0]\n",
    "        while run[0] <= num_steps:\n",
    "\n",
    "            def closure():\n",
    "                # correct the values \n",
    "                # это для того, чтобы значения тензора картинки не выходили за пределы [0;1]\n",
    "                input_img.data.clamp_(0, 1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                model(input_img)\n",
    "\n",
    "                style1_score = 0\n",
    "                style2_score = 0\n",
    "                content_score = 0\n",
    "\n",
    "                for sl in style1_losses:\n",
    "                    style1_score += sl.loss    \n",
    "                for sl in style2_losses:\n",
    "                    style2_score += sl.loss    \n",
    "                for cl in content_losses:\n",
    "                    content_score += cl.loss\n",
    "                \n",
    "                #взвешивание ощибки\n",
    "                style1_score *= style1_weight\n",
    "                style2_score *= style2_weight\n",
    "                content_score *= content_weight\n",
    "\n",
    "                loss = style1_score + style2_score + content_score\n",
    "                loss.backward()\n",
    "\n",
    "                run[0] += 1\n",
    "                if run[0] % 50 == 0:\n",
    "                    print(\"run {}:\".format(run))\n",
    "                    print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
    "                        style_score.item(), content_score.item()))\n",
    "                    print()\n",
    "\n",
    "                return loss\n",
    "\n",
    "            optimizer.step(closure)\n",
    "\n",
    "        # a last correction...\n",
    "        input_img.data.clamp_(0, 1)\n",
    "\n",
    "        return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
